// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.29.0
// source: query.sql

package repository

import (
	"context"
	"time"

	"github.com/jackc/pgx/v5/pgtype"
)

const countDataSources = `-- name: CountDataSources :one
SELECT count(*)
FROM data_sources
WHERE
    ($1::TEXT IS NULL OR name = $1)
AND
    ($2::TEXT IS NULL OR name ILIKE '%' || $2 || '%')
AND deleted_at IS NULL
`

type CountDataSourcesParams struct {
	DataSource pgtype.Text `json:"data_source"`
	Search     pgtype.Text `json:"search"`
}

func (q *Queries) CountDataSources(ctx context.Context, arg CountDataSourcesParams) (int64, error) {
	row := q.db.QueryRow(ctx, countDataSources, arg.DataSource, arg.Search)
	var count int64
	err := row.Scan(&count)
	return count, err
}

const countJobs = `-- name: CountJobs :one
SELECT count(*)
FROM jobs
WHERE
    ($1::TEXT IS NULL OR status = $1)
AND
    ($2::TEXT IS NULL OR id ILIKE '%' || $2 || '%')
`

type CountJobsParams struct {
	Status pgtype.Text `json:"status"`
	Search pgtype.Text `json:"search"`
}

// Count jobs with filtering
func (q *Queries) CountJobs(ctx context.Context, arg CountJobsParams) (int64, error) {
	row := q.db.QueryRow(ctx, countJobs, arg.Status, arg.Search)
	var count int64
	err := row.Scan(&count)
	return count, err
}

const createCrawlerLog = `-- name: CreateCrawlerLog :one

INSERT INTO crawler_logs (id, data_source_id, job_id, event_type, message, details, created_at)
VALUES ($1, $2, $3, $4, $5, $6, $7)
RETURNING id, data_source_id, job_id, event_type, message, details, created_at
`

type CreateCrawlerLogParams struct {
	ID           string      `json:"id"`
	DataSourceID string      `json:"data_source_id"`
	JobID        pgtype.Text `json:"job_id"`
	EventType    string      `json:"event_type"`
	Message      pgtype.Text `json:"message"`
	Details      []byte      `json:"details"`
	CreatedAt    time.Time   `json:"created_at"`
}

// =============================================
// Crawler Logs Table Operations
// =============================================
// Create a new crawler log entry
func (q *Queries) CreateCrawlerLog(ctx context.Context, arg CreateCrawlerLogParams) (CrawlerLog, error) {
	row := q.db.QueryRow(ctx, createCrawlerLog,
		arg.ID,
		arg.DataSourceID,
		arg.JobID,
		arg.EventType,
		arg.Message,
		arg.Details,
		arg.CreatedAt,
	)
	var i CrawlerLog
	err := row.Scan(
		&i.ID,
		&i.DataSourceID,
		&i.JobID,
		&i.EventType,
		&i.Message,
		&i.Details,
		&i.CreatedAt,
	)
	return i, err
}

const createDataSource = `-- name: CreateDataSource :one
INSERT INTO data_sources (id, name, country, source_type, base_url, description, config, is_active, created_at, updated_at)
VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
RETURNING id, name, country, source_type, base_url, description, config, is_active, created_at, updated_at, deleted_at
`

type CreateDataSourceParams struct {
	ID          string      `json:"id"`
	Name        string      `json:"name"`
	Country     string      `json:"country"`
	SourceType  string      `json:"source_type"`
	BaseUrl     pgtype.Text `json:"base_url"`
	Description pgtype.Text `json:"description"`
	Config      []byte      `json:"config"`
	IsActive    bool        `json:"is_active"`
	CreatedAt   time.Time   `json:"created_at"`
	UpdatedAt   time.Time   `json:"updated_at"`
}

// Create a new data source
func (q *Queries) CreateDataSource(ctx context.Context, arg CreateDataSourceParams) (DataSource, error) {
	row := q.db.QueryRow(ctx, createDataSource,
		arg.ID,
		arg.Name,
		arg.Country,
		arg.SourceType,
		arg.BaseUrl,
		arg.Description,
		arg.Config,
		arg.IsActive,
		arg.CreatedAt,
		arg.UpdatedAt,
	)
	var i DataSource
	err := row.Scan(
		&i.ID,
		&i.Name,
		&i.Country,
		&i.SourceType,
		&i.BaseUrl,
		&i.Description,
		&i.Config,
		&i.IsActive,
		&i.CreatedAt,
		&i.UpdatedAt,
		&i.DeletedAt,
	)
	return i, err
}

const createExtractionVersion = `-- name: CreateExtractionVersion :exec

INSERT INTO extraction_versions (
    id, extraction_id, site_content, artifact_link, raw_page_link,
    metadata, page_hash, version, created_at
) VALUES (
    $1, $2, $3, $4, $5, $6, $7, $8, $9
)
`

type CreateExtractionVersionParams struct {
	ID           string      `json:"id"`
	ExtractionID string      `json:"extraction_id"`
	SiteContent  pgtype.Text `json:"site_content"`
	ArtifactLink pgtype.Text `json:"artifact_link"`
	RawPageLink  pgtype.Text `json:"raw_page_link"`
	Metadata     []byte      `json:"metadata"`
	PageHash     pgtype.Text `json:"page_hash"`
	Version      int32       `json:"version"`
	CreatedAt    time.Time   `json:"created_at"`
}

// =============================================
// Extraction Versions Table Operations
// =============================================
// Create a new extraction version
func (q *Queries) CreateExtractionVersion(ctx context.Context, arg CreateExtractionVersionParams) error {
	_, err := q.db.Exec(ctx, createExtractionVersion,
		arg.ID,
		arg.ExtractionID,
		arg.SiteContent,
		arg.ArtifactLink,
		arg.RawPageLink,
		arg.Metadata,
		arg.PageHash,
		arg.Version,
		arg.CreatedAt,
	)
	return err
}

const createJob = `-- name: CreateJob :one

INSERT INTO jobs (
    id,
    status
) VALUES (
    $1, $2
) RETURNING id, status, created_at, updated_at, started_at, finished_at
`

type CreateJobParams struct {
	ID     string `json:"id"`
	Status string `json:"status"`
}

// =============================================
// Jobs Table Operations
// =============================================
// Create a new job (queued)
func (q *Queries) CreateJob(ctx context.Context, arg CreateJobParams) (Job, error) {
	row := q.db.QueryRow(ctx, createJob, arg.ID, arg.Status)
	var i Job
	err := row.Scan(
		&i.ID,
		&i.Status,
		&i.CreatedAt,
		&i.UpdatedAt,
		&i.StartedAt,
		&i.FinishedAt,
	)
	return i, err
}

const deleteDataSource = `-- name: DeleteDataSource :exec
UPDATE data_sources
SET deleted_at = NOW()
WHERE id = $1
`

// Soft delete a data source by ID
func (q *Queries) DeleteDataSource(ctx context.Context, id string) error {
	_, err := q.db.Exec(ctx, deleteDataSource, id)
	return err
}

const getActiveDataSources = `-- name: GetActiveDataSources :many
SELECT id, name, country, source_type, base_url, description, config, is_active, created_at, updated_at, deleted_at
FROM data_sources
WHERE is_active = true AND deleted_at IS NULL
`

// Get all active data sources
func (q *Queries) GetActiveDataSources(ctx context.Context) ([]DataSource, error) {
	rows, err := q.db.Query(ctx, getActiveDataSources)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []DataSource
	for rows.Next() {
		var i DataSource
		if err := rows.Scan(
			&i.ID,
			&i.Name,
			&i.Country,
			&i.SourceType,
			&i.BaseUrl,
			&i.Description,
			&i.Config,
			&i.IsActive,
			&i.CreatedAt,
			&i.UpdatedAt,
			&i.DeletedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getAllDataSources = `-- name: GetAllDataSources :many
SELECT id, name, country, source_type, base_url, description, config, is_active, created_at, updated_at, deleted_at FROM data_sources
`

// Get all data sources, including deleted ones
func (q *Queries) GetAllDataSources(ctx context.Context) ([]DataSource, error) {
	rows, err := q.db.Query(ctx, getAllDataSources)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []DataSource
	for rows.Next() {
		var i DataSource
		if err := rows.Scan(
			&i.ID,
			&i.Name,
			&i.Country,
			&i.SourceType,
			&i.BaseUrl,
			&i.Description,
			&i.Config,
			&i.IsActive,
			&i.CreatedAt,
			&i.UpdatedAt,
			&i.DeletedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getCrawlerLogsByJobId = `-- name: GetCrawlerLogsByJobId :many
SELECT id, data_source_id, job_id, event_type, message, details, created_at FROM crawler_logs WHERE job_id = $1 ORDER BY created_at DESC
`

// Get crawler logs by job ID
func (q *Queries) GetCrawlerLogsByJobId(ctx context.Context, jobID pgtype.Text) ([]CrawlerLog, error) {
	rows, err := q.db.Query(ctx, getCrawlerLogsByJobId, jobID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []CrawlerLog
	for rows.Next() {
		var i CrawlerLog
		if err := rows.Scan(
			&i.ID,
			&i.DataSourceID,
			&i.JobID,
			&i.EventType,
			&i.Message,
			&i.Details,
			&i.CreatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getDataSourceById = `-- name: GetDataSourceById :one

SELECT id, name, country, source_type, base_url, description, config, is_active, created_at, updated_at, deleted_at
FROM data_sources
WHERE id = $1
LIMIT 1
`

// =============================================
// Data Sources Table Operations
// =============================================
// Get data source by ID
func (q *Queries) GetDataSourceById(ctx context.Context, id string) (DataSource, error) {
	row := q.db.QueryRow(ctx, getDataSourceById, id)
	var i DataSource
	err := row.Scan(
		&i.ID,
		&i.Name,
		&i.Country,
		&i.SourceType,
		&i.BaseUrl,
		&i.Description,
		&i.Config,
		&i.IsActive,
		&i.CreatedAt,
		&i.UpdatedAt,
		&i.DeletedAt,
	)
	return i, err
}

const getDataSourceByName = `-- name: GetDataSourceByName :one
SELECT id, name, country, source_type, base_url, description, config, is_active, created_at, updated_at, deleted_at
FROM data_sources
WHERE name = $1 AND deleted_at IS NULL
LIMIT 1
`

// Get data source by name
func (q *Queries) GetDataSourceByName(ctx context.Context, name string) (DataSource, error) {
	row := q.db.QueryRow(ctx, getDataSourceByName, name)
	var i DataSource
	err := row.Scan(
		&i.ID,
		&i.Name,
		&i.Country,
		&i.SourceType,
		&i.BaseUrl,
		&i.Description,
		&i.Config,
		&i.IsActive,
		&i.CreatedAt,
		&i.UpdatedAt,
		&i.DeletedAt,
	)
	return i, err
}

const getExtractionById = `-- name: GetExtractionById :one
SELECT id, url_frontier_id, site_content, artifact_link, raw_page_link,
       extraction_date, content_type, metadata, language, page_hash,
       version, created_at, updated_at
FROM extractions
WHERE id = $1
LIMIT 1
`

// Get extraction by ID
func (q *Queries) GetExtractionById(ctx context.Context, id string) (Extraction, error) {
	row := q.db.QueryRow(ctx, getExtractionById, id)
	var i Extraction
	err := row.Scan(
		&i.ID,
		&i.UrlFrontierID,
		&i.SiteContent,
		&i.ArtifactLink,
		&i.RawPageLink,
		&i.ExtractionDate,
		&i.ContentType,
		&i.Metadata,
		&i.Language,
		&i.PageHash,
		&i.Version,
		&i.CreatedAt,
		&i.UpdatedAt,
	)
	return i, err
}

const getExtractionsByUrlFrontierID = `-- name: GetExtractionsByUrlFrontierID :many
SELECT id, url_frontier_id, site_content, artifact_link, raw_page_link,
       extraction_date, content_type, metadata, language, page_hash,
       version, created_at, updated_at
FROM extractions
WHERE url_frontier_id = $1
ORDER BY version DESC, created_at DESC
`

// Get extractions by URL frontier ID
func (q *Queries) GetExtractionsByUrlFrontierID(ctx context.Context, urlFrontierID string) ([]Extraction, error) {
	rows, err := q.db.Query(ctx, getExtractionsByUrlFrontierID, urlFrontierID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []Extraction
	for rows.Next() {
		var i Extraction
		if err := rows.Scan(
			&i.ID,
			&i.UrlFrontierID,
			&i.SiteContent,
			&i.ArtifactLink,
			&i.RawPageLink,
			&i.ExtractionDate,
			&i.ContentType,
			&i.Metadata,
			&i.Language,
			&i.PageHash,
			&i.Version,
			&i.CreatedAt,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getJobByID = `-- name: GetJobByID :one
SELECT id, status, created_at, updated_at, started_at, finished_at
FROM jobs
WHERE id = $1
LIMIT 1
`

// Get job by ID
func (q *Queries) GetJobByID(ctx context.Context, id string) (Job, error) {
	row := q.db.QueryRow(ctx, getJobByID, id)
	var i Job
	err := row.Scan(
		&i.ID,
		&i.Status,
		&i.CreatedAt,
		&i.UpdatedAt,
		&i.StartedAt,
		&i.FinishedAt,
	)
	return i, err
}

const getUnscrappedUrlFrontiers = `-- name: GetUnscrappedUrlFrontiers :many
SELECT id, data_source_id, domain, url, keyword, priority, status, attempts, last_crawled_at, next_crawl_at, error_message, metadata, created_at, updated_at
FROM url_frontiers
WHERE
  data_source_id = $1
  AND status = $2
ORDER BY priority DESC, created_at ASC LIMIT $3
`

type GetUnscrappedUrlFrontiersParams struct {
	DataSourceID string `json:"data_source_id"`
	Status       int16  `json:"status"`
	Limit        int32  `json:"limit"`
}

// Get unscrapped URL frontiers for a data source
func (q *Queries) GetUnscrappedUrlFrontiers(ctx context.Context, arg GetUnscrappedUrlFrontiersParams) ([]UrlFrontier, error) {
	rows, err := q.db.Query(ctx, getUnscrappedUrlFrontiers, arg.DataSourceID, arg.Status, arg.Limit)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []UrlFrontier
	for rows.Next() {
		var i UrlFrontier
		if err := rows.Scan(
			&i.ID,
			&i.DataSourceID,
			&i.Domain,
			&i.Url,
			&i.Keyword,
			&i.Priority,
			&i.Status,
			&i.Attempts,
			&i.LastCrawledAt,
			&i.NextCrawlAt,
			&i.ErrorMessage,
			&i.Metadata,
			&i.CreatedAt,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getUrlFrontierById = `-- name: GetUrlFrontierById :one
SELECT id, data_source_id, domain, url, keyword, priority, status, attempts, last_crawled_at, next_crawl_at, error_message, metadata, created_at, updated_at
FROM url_frontiers
WHERE id = $1
LIMIT 1
`

// Get URL frontier by ID
func (q *Queries) GetUrlFrontierById(ctx context.Context, id string) (UrlFrontier, error) {
	row := q.db.QueryRow(ctx, getUrlFrontierById, id)
	var i UrlFrontier
	err := row.Scan(
		&i.ID,
		&i.DataSourceID,
		&i.Domain,
		&i.Url,
		&i.Keyword,
		&i.Priority,
		&i.Status,
		&i.Attempts,
		&i.LastCrawledAt,
		&i.NextCrawlAt,
		&i.ErrorMessage,
		&i.Metadata,
		&i.CreatedAt,
		&i.UpdatedAt,
	)
	return i, err
}

const getUrlFrontierByUrl = `-- name: GetUrlFrontierByUrl :one
SELECT id, data_source_id, domain, url, keyword, priority, status, attempts, last_crawled_at, next_crawl_at, error_message, metadata, created_at, updated_at
FROM url_frontiers
WHERE url = $1
LIMIT 1
`

// Get URL frontier by URL
func (q *Queries) GetUrlFrontierByUrl(ctx context.Context, url string) (UrlFrontier, error) {
	row := q.db.QueryRow(ctx, getUrlFrontierByUrl, url)
	var i UrlFrontier
	err := row.Scan(
		&i.ID,
		&i.DataSourceID,
		&i.Domain,
		&i.Url,
		&i.Keyword,
		&i.Priority,
		&i.Status,
		&i.Attempts,
		&i.LastCrawledAt,
		&i.NextCrawlAt,
		&i.ErrorMessage,
		&i.Metadata,
		&i.CreatedAt,
		&i.UpdatedAt,
	)
	return i, err
}

const listDataSources = `-- name: ListDataSources :many
SELECT id, name, country, source_type, base_url, description, config, is_active, created_at, updated_at, deleted_at
FROM data_sources
WHERE
    ($1::TEXT IS NULL OR name = $1)
AND
    ($2::TEXT IS NULL OR name ILIKE '%' || $2 || '%')
AND deleted_at IS NULL
ORDER BY
    name ASC
LIMIT $4
OFFSET $3
`

type ListDataSourcesParams struct {
	DataSource pgtype.Text `json:"data_source"`
	Search     pgtype.Text `json:"search"`
	Offset     int32       `json:"offset"`
	Limit      int32       `json:"limit"`
}

func (q *Queries) ListDataSources(ctx context.Context, arg ListDataSourcesParams) ([]DataSource, error) {
	rows, err := q.db.Query(ctx, listDataSources,
		arg.DataSource,
		arg.Search,
		arg.Offset,
		arg.Limit,
	)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []DataSource
	for rows.Next() {
		var i DataSource
		if err := rows.Scan(
			&i.ID,
			&i.Name,
			&i.Country,
			&i.SourceType,
			&i.BaseUrl,
			&i.Description,
			&i.Config,
			&i.IsActive,
			&i.CreatedAt,
			&i.UpdatedAt,
			&i.DeletedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const listJobs = `-- name: ListJobs :many
SELECT id, status, created_at, updated_at, started_at, finished_at
FROM jobs
WHERE
    ($1::TEXT IS NULL OR status = $1)
AND
    ($2::TEXT IS NULL OR id ILIKE '%' || $2 || '%')
ORDER BY created_at DESC
LIMIT $4
OFFSET $3
`

type ListJobsParams struct {
	Status pgtype.Text `json:"status"`
	Search pgtype.Text `json:"search"`
	Offset int32       `json:"offset"`
	Limit  int32       `json:"limit"`
}

// List jobs with pagination and filtering
func (q *Queries) ListJobs(ctx context.Context, arg ListJobsParams) ([]Job, error) {
	rows, err := q.db.Query(ctx, listJobs,
		arg.Status,
		arg.Search,
		arg.Offset,
		arg.Limit,
	)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []Job
	for rows.Next() {
		var i Job
		if err := rows.Scan(
			&i.ID,
			&i.Status,
			&i.CreatedAt,
			&i.UpdatedAt,
			&i.StartedAt,
			&i.FinishedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const updateDataSource = `-- name: UpdateDataSource :one
UPDATE data_sources
SET
  name = $2,
  country = $3,
  source_type = $4,
  base_url = $5,
  description = $6,
  config = $7,
  is_active = $8,
  updated_at = $9
WHERE id = $1
RETURNING id, name, country, source_type, base_url, description, config, is_active, created_at, updated_at, deleted_at
`

type UpdateDataSourceParams struct {
	ID          string      `json:"id"`
	Name        string      `json:"name"`
	Country     string      `json:"country"`
	SourceType  string      `json:"source_type"`
	BaseUrl     pgtype.Text `json:"base_url"`
	Description pgtype.Text `json:"description"`
	Config      []byte      `json:"config"`
	IsActive    bool        `json:"is_active"`
	UpdatedAt   time.Time   `json:"updated_at"`
}

// Update an existing data source
func (q *Queries) UpdateDataSource(ctx context.Context, arg UpdateDataSourceParams) (DataSource, error) {
	row := q.db.QueryRow(ctx, updateDataSource,
		arg.ID,
		arg.Name,
		arg.Country,
		arg.SourceType,
		arg.BaseUrl,
		arg.Description,
		arg.Config,
		arg.IsActive,
		arg.UpdatedAt,
	)
	var i DataSource
	err := row.Scan(
		&i.ID,
		&i.Name,
		&i.Country,
		&i.SourceType,
		&i.BaseUrl,
		&i.Description,
		&i.Config,
		&i.IsActive,
		&i.CreatedAt,
		&i.UpdatedAt,
		&i.DeletedAt,
	)
	return i, err
}

const updateJobStatus = `-- name: UpdateJobStatus :one
UPDATE jobs
SET
    status = $2,
    updated_at = NOW()
WHERE id = $1
RETURNING id, status, created_at, updated_at, started_at, finished_at
`

type UpdateJobStatusParams struct {
	ID     string `json:"id"`
	Status string `json:"status"`
}

// Update job status and updated_at timestamp
func (q *Queries) UpdateJobStatus(ctx context.Context, arg UpdateJobStatusParams) (Job, error) {
	row := q.db.QueryRow(ctx, updateJobStatus, arg.ID, arg.Status)
	var i Job
	err := row.Scan(
		&i.ID,
		&i.Status,
		&i.CreatedAt,
		&i.UpdatedAt,
		&i.StartedAt,
		&i.FinishedAt,
	)
	return i, err
}

const updateUrlFrontierStatus = `-- name: UpdateUrlFrontierStatus :exec
UPDATE url_frontiers
SET
  status = $2,
  attempts = COALESCE(attempts, 0) + 1,
  last_crawled_at = CURRENT_TIMESTAMP,
  error_message = $3,
  updated_at = $4
WHERE id = $1
`

type UpdateUrlFrontierStatusParams struct {
	ID           string      `json:"id"`
	Status       int16       `json:"status"`
	ErrorMessage pgtype.Text `json:"error_message"`
	UpdatedAt    time.Time   `json:"updated_at"`
}

// Update status and increment attempts for URL frontiers
func (q *Queries) UpdateUrlFrontierStatus(ctx context.Context, arg UpdateUrlFrontierStatusParams) error {
	_, err := q.db.Exec(ctx, updateUrlFrontierStatus,
		arg.ID,
		arg.Status,
		arg.ErrorMessage,
		arg.UpdatedAt,
	)
	return err
}

const updateUrlFrontierStatusBatch = `-- name: UpdateUrlFrontierStatusBatch :exec
UPDATE url_frontiers
SET
  status = $2,
  attempts = COALESCE(attempts, 0) + 1,
  last_crawled_at = CURRENT_TIMESTAMP,
  error_message = $3,
  updated_at = $4
WHERE id = ANY($1)
`

type UpdateUrlFrontierStatusBatchParams struct {
	ID           string      `json:"id"`
	Status       int16       `json:"status"`
	ErrorMessage pgtype.Text `json:"error_message"`
	UpdatedAt    time.Time   `json:"updated_at"`
}

// Update Status and increment by batch ids
func (q *Queries) UpdateUrlFrontierStatusBatch(ctx context.Context, arg UpdateUrlFrontierStatusBatchParams) error {
	_, err := q.db.Exec(ctx, updateUrlFrontierStatusBatch,
		arg.ID,
		arg.Status,
		arg.ErrorMessage,
		arg.UpdatedAt,
	)
	return err
}

const upsertDataSource = `-- name: UpsertDataSource :exec
INSERT INTO data_sources (id, name, country, source_type, base_url, description, config, is_active, created_at, updated_at)
VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
ON CONFLICT (id) DO UPDATE
SET
  name = $2,
  country = $3,
  source_type = $4,
  base_url = $5,
  description = $6,
  config = $7,
  is_active = $8,
  updated_at = $10
`

type UpsertDataSourceParams struct {
	ID          string      `json:"id"`
	Name        string      `json:"name"`
	Country     string      `json:"country"`
	SourceType  string      `json:"source_type"`
	BaseUrl     pgtype.Text `json:"base_url"`
	Description pgtype.Text `json:"description"`
	Config      []byte      `json:"config"`
	IsActive    bool        `json:"is_active"`
	CreatedAt   time.Time   `json:"created_at"`
	UpdatedAt   time.Time   `json:"updated_at"`
}

// Upsert data source
func (q *Queries) UpsertDataSource(ctx context.Context, arg UpsertDataSourceParams) error {
	_, err := q.db.Exec(ctx, upsertDataSource,
		arg.ID,
		arg.Name,
		arg.Country,
		arg.SourceType,
		arg.BaseUrl,
		arg.Description,
		arg.Config,
		arg.IsActive,
		arg.CreatedAt,
		arg.UpdatedAt,
	)
	return err
}

const upsertUrlFrontier = `-- name: UpsertUrlFrontier :exec

INSERT INTO url_frontiers (id, data_source_id, domain, url, keyword, priority, status, attempts, last_crawled_at, next_crawl_at, error_message, metadata, created_at, updated_at)
VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14)
ON CONFLICT (url, data_source_id) DO UPDATE
SET
  domain = $3,
  keyword = $5,
  priority = $6,
  status = $7,
  attempts = $8,
  last_crawled_at = $9,
  next_crawl_at = $10,
  error_message = $11,
  metadata = $12,
  updated_at = $14
`

type UpsertUrlFrontierParams struct {
	ID            string             `json:"id"`
	DataSourceID  string             `json:"data_source_id"`
	Domain        string             `json:"domain"`
	Url           string             `json:"url"`
	Keyword       pgtype.Text        `json:"keyword"`
	Priority      int16              `json:"priority"`
	Status        int16              `json:"status"`
	Attempts      int16              `json:"attempts"`
	LastCrawledAt pgtype.Timestamptz `json:"last_crawled_at"`
	NextCrawlAt   pgtype.Timestamptz `json:"next_crawl_at"`
	ErrorMessage  pgtype.Text        `json:"error_message"`
	Metadata      []byte             `json:"metadata"`
	CreatedAt     time.Time          `json:"created_at"`
	UpdatedAt     time.Time          `json:"updated_at"`
}

// =============================================
// URL Frontiers Table Operations
// =============================================
// Upsert a single URL frontier record
func (q *Queries) UpsertUrlFrontier(ctx context.Context, arg UpsertUrlFrontierParams) error {
	_, err := q.db.Exec(ctx, upsertUrlFrontier,
		arg.ID,
		arg.DataSourceID,
		arg.Domain,
		arg.Url,
		arg.Keyword,
		arg.Priority,
		arg.Status,
		arg.Attempts,
		arg.LastCrawledAt,
		arg.NextCrawlAt,
		arg.ErrorMessage,
		arg.Metadata,
		arg.CreatedAt,
		arg.UpdatedAt,
	)
	return err
}
